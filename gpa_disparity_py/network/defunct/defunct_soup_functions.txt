from bs4 import BeautifulSoup
from bs4 import ResultSet
import beautifulsoup_parsers
import requests

def get_soup(url: str) -> BeautifulSoup:
    """
    takes in a url and returns a soup to be parsed by BeautifulSoup
    Args:
        url (str):  url to the address to read
    """
    #https://www.crummy.com/software/BeautifulSoup/bs4/doc/#making-the-soup
    #https://www.dataquest.io/blog/web-scraping-python-using-beautiful-soup/

    page = requests.get(url)

    assert page.status_code == 200

    soup = BeautifulSoup(page.content, beautifulsoup_parsers.html)

    return soup

def write_soup_to_file(url:str):
    """writes the requested webpage to file in a prettiefied format.
    Intended for debug / development purposes

    Args:
        url (str): _description_
    """
    soup = get_soup(url)
    f = open("./gpa_disparity_py/network/temp_webpage.txt", "w")
    f.write(url + "\n")
    f.write(soup.prettify())
    f.close

def get_links(url:str) -> ResultSet:
    soup = get_soup(url)
    for link in soup.find_all('a'):
        print(link.get('href'))
    return ResultSet

write_soup_to_file(data_sources.gpa_url)

get_links(data_sources.gpa_url)